import os
import csv
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import re
from typing import List, Dict, Tuple, Optional, Union
import json
import random
import numpy as np
from torch.utils.data import WeightedRandomSampler
import unicodedata


class AdvancedTextProcessor:
    """ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®é«˜åº¦ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†"""
    
    def __init__(self, vocab_size: int = 32000):
        self.vocab_size = vocab_size
        self.char_to_id = {}
        self.id_to_char = {}
        self.char_freqs = Counter()
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.special_tokens = {
            '<pad>': 0,
            '<eos>': 1, 
            '<bos>': 2,
            '<unk>': 3,
            '<mask>': 4,
            '<sep>': 5,
        }
        
        # è¨€èªæ¤œå‡ºç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.language_patterns = {
            'japanese': re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]'),
            'english': re.compile(r'[a-zA-Z]'),
            'python': re.compile(r'(def |class |import |from |if |for |while |try |except)')
        }
        
    def normalize_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ï¼ˆç²¾åº¦å‘ä¸Šã®ãŸã‚ï¼‰"""
        # Unicodeæ­£è¦åŒ–
        text = unicodedata.normalize('NFKC', text)
        
        # ç©ºç™½æ–‡å­—ã®çµ±ä¸€
        text = re.sub(r'\s+', ' ', text)
        
        # åˆ¶å¾¡æ–‡å­—ã®é™¤å»
        text = ''.join(char for char in text if unicodedata.category(char)[0] != 'C')
        
        return text.strip()
    
    def detect_language(self, text: str) -> str:
        """è¨€èªæ¤œå‡º"""
        scores = {}
        for lang, pattern in self.language_patterns.items():
            matches = len(pattern.findall(text))
            scores[lang] = matches / len(text) if text else 0
        
        return max(scores, key=scores.get) if scores else 'unknown'
    
    def build_vocab(self, texts: List[str]) -> None:
        """æ”¹è‰¯ã•ã‚ŒãŸèªå½™æ§‹ç¯‰"""
        # æ–‡å­—é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for text in texts:
            normalized_text = self.normalize_text(text)
            self.char_freqs.update(normalized_text)
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’æœ€åˆã«è¿½åŠ 
        self.char_to_id.update(self.special_tokens)
        self.id_to_char.update({v: k for k, v in self.special_tokens.items()})
        
        # é »åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_chars = sorted(self.char_freqs.items(), key=lambda x: x[1], reverse=True)
        
        # èªå½™ã‚µã‚¤ã‚ºã¾ã§è¿½åŠ 
        current_id = len(self.special_tokens)
        for char, freq in sorted_chars:
            if current_id >= self.vocab_size:
                break
            if char not in self.char_to_id:
                self.char_to_id[char] = current_id
                self.id_to_char[current_id] = char
                current_id += 1
        
        print(f"ğŸ”¤ èªå½™æ§‹ç¯‰å®Œäº†: {len(self.char_to_id)} tokens")
    
    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›"""
        text = self.normalize_text(text)
        
        tokens = []
        if add_special_tokens:
            tokens.append(self.special_tokens['<bos>'])
        
        for char in text:
            tokens.append(self.char_to_id.get(char, self.special_tokens['<unk>']))
        
        if add_special_tokens:
            tokens.append(self.special_tokens['<eos>'])
            
        return tokens
    
    def decode(self, token_ids: List[int]) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        chars = []
        for token_id in token_ids:
            char = self.id_to_char.get(token_id, '<unk>')
            if char not in ['<pad>', '<bos>', '<eos>']:
                chars.append(char)
        return ''.join(chars)


class DataAugmentation:
    """ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
    
    def __init__(self, noise_prob: float = 0.1, mask_prob: float = 0.15):
        self.noise_prob = noise_prob
        self.mask_prob = mask_prob
    
    def add_noise(self, text: str, processor: AdvancedTextProcessor) -> str:
        """ãƒã‚¤ã‚ºè¿½åŠ """
        if random.random() > self.noise_prob:
            return text
            
        chars = list(text)
        num_noise = max(1, int(len(chars) * 0.05))  # 5%ã®æ–‡å­—ã«ãƒã‚¤ã‚º
        
        for _ in range(num_noise):
            if not chars:
                break
                
            pos = random.randint(0, len(chars) - 1)
            operation = random.choice(['replace', 'delete', 'insert'])
            
            if operation == 'replace' and chars:
                # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡å­—ã§ç½®æ›
                random_char = random.choice(list(processor.char_to_id.keys()))
                chars[pos] = random_char
            elif operation == 'delete' and len(chars) > 1:
                # å‰Šé™¤
                chars.pop(pos)
            elif operation == 'insert':
                # æŒ¿å…¥
                random_char = random.choice(list(processor.char_to_id.keys()))
                chars.insert(pos, random_char)
        
        return ''.join(chars)
    
    def mask_tokens(self, token_ids: List[int], mask_token_id: int) -> Tuple[List[int], List[int]]:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¹ã‚­ãƒ³ã‚°ï¼ˆBERT styleï¼‰"""
        if len(token_ids) < 2:
            return token_ids, token_ids
            
        masked_ids = token_ids.copy()
        labels = [-100] * len(token_ids)  # -100 = ignore in loss
        
        # ãƒã‚¹ã‚¯å¯¾è±¡ã‚’é¸æŠ
        mask_indices = []
        for i, token_id in enumerate(token_ids):
            if token_id not in [0, 1, 2] and random.random() < self.mask_prob:  # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã
                mask_indices.append(i)
        
        for i in mask_indices:
            labels[i] = token_ids[i]  # æ­£è§£ãƒ©ãƒ™ãƒ«
            
            prob = random.random()
            if prob < 0.8:
                # 80%ã®ç¢ºç‡ã§ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›
                masked_ids[i] = mask_token_id
            elif prob < 0.9:
                # 10%ã®ç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›
                masked_ids[i] = random.randint(6, len(masked_ids) - 1)
            # 10%ã®ç¢ºç‡ã§ãã®ã¾ã¾ï¼ˆä½•ã‚‚ã—ãªã„ï¼‰
        
        return masked_ids, labels


class ImprovedTextDataset(Dataset):
    """ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®æ”¹è‰¯ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir: str, max_length: int = 512, 
                 use_augmentation: bool = True, balance_languages: bool = True):
        self.data_dir = data_dir
        self.max_length = max_length
        self.use_augmentation = use_augmentation
        self.balance_languages = balance_languages
        
        # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã¨æ‹¡å¼µå™¨
        self.processor = AdvancedTextProcessor()
        self.augmenter = DataAugmentation() if use_augmentation else None
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.texts, self.labels, self.languages = self._load_data()
        
        # èªå½™æ§‹ç¯‰
        self.processor.build_vocab(self.texts)
        
        # è¨€èªãƒãƒ©ãƒ³ã‚¹èª¿æ•´ç”¨ã®é‡ã¿è¨ˆç®—
        self.sample_weights = self._compute_sample_weights() if balance_languages else None
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:")
        print(f"  - ç·ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(self.texts)}")
        print(f"  - è¨€èªåˆ†å¸ƒ: {Counter(self.languages)}")
        print(f"  - å¹³å‡é•·: {np.mean([len(text) for text in self.texts]):.1f}")
        
    def _load_data(self) -> Tuple[List[str], List[str], List[str]]:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        texts = []
        labels = []
        languages = []
        
        if not os.path.exists(self.data_dir):
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.data_dir}")
            return [], [], []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        for filename in os.listdir(self.data_dir):
            filepath = os.path.join(self.data_dir, filename)
            
            if not os.path.isfile(filepath):
                continue
                
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’é©åˆ‡ãªé•·ã•ã«åˆ†å‰²
                chunks = self._split_text(content)
                
                for chunk in chunks:
                    if len(chunk.strip()) < 10:  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                        continue
                        
                    language = self.processor.detect_language(chunk)
                    
                    texts.append(chunk)
                    labels.append(filename)
                    languages.append(language)
                    
            except Exception as e:
                print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {filepath} - {e}")
                continue
        
        return texts, labels, languages
    
    def _split_text(self, text: str, overlap: int = 50) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªé•·ã•ã«åˆ†å‰²ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä»˜ãï¼‰"""
        if len(text) <= self.max_length:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.max_length
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # æ–‡å¢ƒç•Œã§åˆ†å‰²ã‚’è©¦è¡Œ
            chunk = text[start:end]
            last_period = chunk.rfind('ã€‚')
            last_newline = chunk.rfind('\n')
            last_space = chunk.rfind(' ')
            
            split_pos = max(last_period, last_newline, last_space)
            
            if split_pos > start + self.max_length // 2:
                end = start + split_pos + 1
            
            chunks.append(text[start:end])
            start = end - overlap  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        
        return chunks
    
    def _compute_sample_weights(self) -> List[float]:
        """è¨€èªãƒãƒ©ãƒ³ã‚¹èª¿æ•´ç”¨ã®é‡ã¿è¨ˆç®—"""
        language_counts = Counter(self.languages)
        total_samples = len(self.languages)
        
        # å„è¨€èªã®é‡ã¿ã‚’è¨ˆç®—ï¼ˆé€†é »åº¦é‡ã¿ä»˜ã‘ï¼‰
        language_weights = {}
        for lang, count in language_counts.items():
            language_weights[lang] = total_samples / (len(language_counts) * count)
        
        # ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®é‡ã¿ã‚’è¨ˆç®—
        weights = [language_weights[lang] for lang in self.languages]
        return weights
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        language = self.languages[idx]
        
        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆè¨“ç·´æ™‚ã®ã¿ï¼‰
        if self.augmenter and self.use_augmentation and random.random() < 0.3:
            text = self.augmenter.add_noise(text, self.processor)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        token_ids = self.processor.encode(text)
        
        # é•·ã•èª¿æ•´
        if len(token_ids) > self.max_length:
            token_ids = token_ids[:self.max_length]
        else:
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            token_ids += [0] * (self.max_length - len(token_ids))
        
        # ãƒã‚¹ã‚­ãƒ³ã‚°ï¼ˆç¢ºç‡çš„ã«é©ç”¨ï¼‰
        masked_ids = token_ids.copy()
        if self.augmenter and random.random() < 0.2:  # 20%ã®ç¢ºç‡
            masked_ids, _ = self.augmenter.mask_tokens(token_ids, self.processor.special_tokens['<mask>'])
        
        return {
            'input_ids': torch.tensor(token_ids, dtype=torch.long),
            'labels': torch.tensor(token_ids, dtype=torch.long),  # è‡ªå·±æ•™å¸«å­¦ç¿’
            'language': language,
            'length': len([t for t in token_ids if t != 0])  # å®Ÿéš›ã®é•·ã•
        }


def create_balanced_dataloader(dataset: ImprovedTextDataset, batch_size: int = 16, 
                             use_weighted_sampling: bool = True) -> DataLoader:
    """ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
    
    if use_weighted_sampling and dataset.sample_weights:
        # é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        sampler = WeightedRandomSampler(
            weights=dataset.sample_weights,
            num_samples=len(dataset),
            replacement=True
        )
        shuffle = False
    else:
        sampler = None
        shuffle = True
    
    # å‹•çš„ãƒãƒƒãƒå‡¦ç†ç”¨ã®collateé–¢æ•°
    def collate_fn(batch):
        # é•·ã•ã§ã‚½ãƒ¼ãƒˆï¼ˆåŠ¹ç‡çš„ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ãŸã‚ï¼‰
        batch = sorted(batch, key=lambda x: x['length'], reverse=True)
        
        # ãƒãƒƒãƒå†…ã®æœ€å¤§é•·ã«åˆã‚ã›ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°èª¿æ•´
        max_len = max(item['length'] for item in batch)
        max_len = min(max_len, dataset.max_length)  # æœ€å¤§é•·åˆ¶é™
        
        input_ids = []
        labels = []
        languages = []
        lengths = []
        
        for item in batch:
            # å¿…è¦ã«å¿œã˜ã¦ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
            input_id = item['input_ids'][:max_len]
            label = item['labels'][:max_len]
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            if len(input_id) < max_len:
                padding_size = max_len - len(input_id)
                input_id = torch.cat([input_id, torch.zeros(padding_size, dtype=torch.long)])
                label = torch.cat([label, torch.full((padding_size,), -100, dtype=torch.long)])
            
            input_ids.append(input_id)
            labels.append(label)
            languages.append(item['language'])
            lengths.append(item['length'])
        
        return {
            'input_ids': torch.stack(input_ids),
            'labels': torch.stack(labels),
            'languages': languages,
            'lengths': torch.tensor(lengths)
        }
    
    return DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=shuffle,
        sampler=sampler,
        collate_fn=collate_fn,
        num_workers=0,  # Windowsäº’æ›æ€§ã®ãŸã‚0ã«è¨­å®š
        pin_memory=False  # CPUä½¿ç”¨æ™‚ã¯Falseã«è¨­å®š
    )


def create_sample_data(data_dir: str = "sample_data"):
    """æ”¹è‰¯ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    os.makedirs(data_dir, exist_ok=True)
    
    # ã‚ˆã‚Šè±Šå¯Œãªæ—¥æœ¬èªã‚µãƒ³ãƒ—ãƒ«
    japanese_samples = [
        "æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¤šå±¤ã«ç©ã¿é‡ã­ãŸæ‰‹æ³•ã§ã€ç”»åƒèªè­˜ã‚„è‡ªç„¶è¨€èªå‡¦ç†ã«å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚",
        "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã„ã¦é©æ–°çš„ãªé€²æ­©ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚",
        "Mambaãƒ¢ãƒ‡ãƒ«ã¯çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡¦ç†ã«åŠ¹ç‡çš„ã§ã™ã€‚",
        "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®åé›†ã€å‰å‡¦ç†ã€åˆ†æã€å¯è¦–åŒ–ãŒé‡è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚",
        "å¼·åŒ–å­¦ç¿’ã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚",
        "ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹é‡è¦ãªæ‰‹æ³•ã®ä¸€ã¤ã§ã™ã€‚",
        "ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’é˜²ããŸã‚ã«ã€æ­£å‰‡åŒ–ã‚„ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆãªã©ã®æŠ€è¡“ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚"
    ]
    
    # ã‚ˆã‚Šè±Šå¯Œãªè‹±èªã‚µãƒ³ãƒ—ãƒ«
    english_samples = [
        "Machine learning is a subset of artificial intelligence that focuses on algorithms and statistical models.",
        "Deep learning models have achieved remarkable success in computer vision and natural language processing tasks.",
        "The transformer architecture revolutionized the field of natural language processing with attention mechanisms.",
        "State space models like Mamba offer efficient alternatives to transformers for long sequence modeling.",
        "Data preprocessing is a crucial step that significantly impacts the performance of machine learning models.",
        "Regularization techniques help prevent overfitting and improve model generalization.",
        "Cross-validation provides robust estimates of model performance across different data splits.",
        "Feature engineering involves creating meaningful input variables that improve model accuracy."
    ]
    
    # ã‚ˆã‚Šè±Šå¯ŒãªPythonã‚µãƒ³ãƒ—ãƒ«
    python_samples = [
        '''
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        return self.layers(x)
''',
        '''
def train_model(model, train_loader, optimizer, criterion):
    model.train()
    total_loss = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(train_loader)
''',
        '''
class DataProcessor:
    def __init__(self, vocab_size=10000):
        self.vocab_size = vocab_size
        self.word_to_id = {}
        self.id_to_word = {}
    
    def build_vocab(self, texts):
        word_counts = {}
        for text in texts:
            for word in text.split():
                word_counts[word] = word_counts.get(word, 0) + 1
        
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        for i, (word, count) in enumerate(sorted_words[:self.vocab_size]):
            self.word_to_id[word] = i
            self.id_to_word[i] = word
'''
    ]
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(os.path.join(data_dir, "japanese.txt"), "w", encoding="utf-8") as f:
        f.write("\n\n".join(japanese_samples))
    
    with open(os.path.join(data_dir, "english.txt"), "w", encoding="utf-8") as f:
        f.write("\n\n".join(english_samples))
    
    with open(os.path.join(data_dir, "sample_code.py"), "w", encoding="utf-8") as f:
        f.write("\n\n".join(python_samples))
    
    print(f"ğŸ“ æ”¹è‰¯ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸ: {data_dir}")