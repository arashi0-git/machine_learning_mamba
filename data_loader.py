import os
import csv
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import re
from typing import List, Dict, Tuple, Optional, Union
import json
import random
import numpy as np
from torch.utils.data import WeightedRandomSampler
import unicodedata


class AdvancedTextProcessor:
    """ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®é«˜åº¦ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†"""
    
    def __init__(self, vocab_size: int = 2000):
        self.vocab_size = vocab_size
        self.char_to_id = {}
        self.id_to_char = {}
        self.char_freqs = Counter()
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.special_tokens = {
            '<pad>': 0,
            '<eos>': 1, 
            '<bos>': 2,
            '<unk>': 3,
            '<mask>': 4,
            '<sep>': 5,
        }
        
        # è¨€èªæ¤œå‡ºç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.language_patterns = {
            'japanese': re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]'),
            'english': re.compile(r'[a-zA-Z]'),
            'python': re.compile(r'(def |class |import |from |if |for |while |try |except)')
        }
        
    def normalize_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ï¼ˆç²¾åº¦å‘ä¸Šã®ãŸã‚ï¼‰"""
        # Unicodeæ­£è¦åŒ–
        text = unicodedata.normalize('NFKC', text)
        
        # ç©ºç™½æ–‡å­—ã®çµ±ä¸€
        text = re.sub(r'\s+', ' ', text)
        
        # åˆ¶å¾¡æ–‡å­—ã®é™¤å»
        text = ''.join(char for char in text if unicodedata.category(char)[0] != 'C')
        
        return text.strip()
    
    def detect_language(self, text: str) -> str:
        """è¨€èªæ¤œå‡º"""
        scores = {}
        for lang, pattern in self.language_patterns.items():
            matches = len(pattern.findall(text))
            scores[lang] = matches / len(text) if text else 0
        
        return max(scores, key=scores.get) if scores else 'unknown'
    
    def build_vocab(self, texts: List[str]) -> None:
        """æ”¹è‰¯ã•ã‚ŒãŸèªå½™æ§‹ç¯‰"""
        # æ–‡å­—é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for text in texts:
            normalized_text = self.normalize_text(text)
            self.char_freqs.update(normalized_text)
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’æœ€åˆã«è¿½åŠ 
        self.char_to_id.update(self.special_tokens)
        self.id_to_char.update({v: k for k, v in self.special_tokens.items()})
        
        # é »åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_chars = sorted(self.char_freqs.items(), key=lambda x: x[1], reverse=True)
        
        # èªå½™ã‚µã‚¤ã‚ºã¾ã§è¿½åŠ 
        current_id = len(self.special_tokens)
        for char, freq in sorted_chars:
            if current_id >= self.vocab_size:
                break
            if char not in self.char_to_id:
                self.char_to_id[char] = current_id
                self.id_to_char[current_id] = char
                current_id += 1
        
        print(f"ğŸ”¤ èªå½™æ§‹ç¯‰å®Œäº†: {len(self.char_to_id)} tokens")
    
    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›"""
        text = self.normalize_text(text)
        
        tokens = []
        if add_special_tokens:
            tokens.append(self.special_tokens['<bos>'])
        
        for char in text:
            tokens.append(self.char_to_id.get(char, self.special_tokens['<unk>']))
        
        if add_special_tokens:
            tokens.append(self.special_tokens['<eos>'])
            
        return tokens
    
    def decode(self, token_ids: List[int]) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        chars = []
        for token_id in token_ids:
            char = self.id_to_char.get(token_id, '<unk>')
            if char not in ['<pad>', '<bos>', '<eos>']:
                chars.append(char)
        return ''.join(chars)


class DataAugmentation:
    """ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
    
    def __init__(self, noise_prob: float = 0.1, mask_prob: float = 0.15):
        self.noise_prob = noise_prob
        self.mask_prob = mask_prob
    
    def add_noise(self, text: str, processor: AdvancedTextProcessor) -> str:
        """ãƒã‚¤ã‚ºè¿½åŠ """
        if random.random() > self.noise_prob:
            return text
            
        chars = list(text)
        num_noise = max(1, int(len(chars) * 0.05))  # 5%ã®æ–‡å­—ã«ãƒã‚¤ã‚º
        
        for _ in range(num_noise):
            if not chars:
                break
                
            pos = random.randint(0, len(chars) - 1)
            operation = random.choice(['replace', 'delete', 'insert'])
            
            if operation == 'replace' and chars:
                # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡å­—ã§ç½®æ›
                random_char = random.choice(list(processor.char_to_id.keys()))
                chars[pos] = random_char
            elif operation == 'delete' and len(chars) > 1:
                # å‰Šé™¤
                chars.pop(pos)
            elif operation == 'insert':
                # æŒ¿å…¥
                random_char = random.choice(list(processor.char_to_id.keys()))
                chars.insert(pos, random_char)
        
        return ''.join(chars)
    
    def mask_tokens(self, token_ids: List[int], mask_token_id: int) -> Tuple[List[int], List[int]]:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¹ã‚­ãƒ³ã‚°ï¼ˆBERT styleï¼‰"""
        if len(token_ids) < 2:
            return token_ids, token_ids
            
        masked_ids = token_ids.copy()
        labels = [-100] * len(token_ids)  # -100 = ignore in loss
        
        # ãƒã‚¹ã‚¯å¯¾è±¡ã‚’é¸æŠ
        mask_indices = []
        for i, token_id in enumerate(token_ids):
            if token_id not in [0, 1, 2] and random.random() < self.mask_prob:  # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã
                mask_indices.append(i)
        
        for i in mask_indices:
            labels[i] = token_ids[i]  # æ­£è§£ãƒ©ãƒ™ãƒ«
            
            prob = random.random()
            if prob < 0.8:
                # 80%ã®ç¢ºç‡ã§ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›
                masked_ids[i] = mask_token_id
            elif prob < 0.9:
                # 10%ã®ç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›
                masked_ids[i] = random.randint(6, len(masked_ids) - 1)
            # 10%ã®ç¢ºç‡ã§ãã®ã¾ã¾ï¼ˆä½•ã‚‚ã—ãªã„ï¼‰
        
        return masked_ids, labels


class ImprovedTextDataset(Dataset):
    """ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®æ”¹è‰¯ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir: str, max_length: int = 512, 
                 use_augmentation: bool = True, balance_languages: bool = True):
        self.data_dir = data_dir
        self.max_length = max_length
        self.use_augmentation = use_augmentation
        self.balance_languages = balance_languages
        
        # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã¨æ‹¡å¼µå™¨
        self.processor = AdvancedTextProcessor()
        self.augmenter = DataAugmentation() if use_augmentation else None
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.texts, self.labels, self.languages = self._load_data()
        
        # èªå½™æ§‹ç¯‰
        self.processor.build_vocab(self.texts)
        
        # è¨€èªãƒãƒ©ãƒ³ã‚¹èª¿æ•´ç”¨ã®é‡ã¿è¨ˆç®—
        self.sample_weights = self._compute_sample_weights() if balance_languages else None
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:")
        print(f"  - ç·ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(self.texts)}")
        print(f"  - è¨€èªåˆ†å¸ƒ: {Counter(self.languages)}")
        print(f"  - å¹³å‡é•·: {np.mean([len(text) for text in self.texts]):.1f}")
        
    def _load_data(self) -> Tuple[List[str], List[str], List[str]]:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        texts = []
        labels = []
        languages = []
        
        if not os.path.exists(self.data_dir):
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.data_dir}")
            return [], [], []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        for filename in os.listdir(self.data_dir):
            filepath = os.path.join(self.data_dir, filename)
            
            if not os.path.isfile(filepath):
                continue
                
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’é©åˆ‡ãªé•·ã•ã«åˆ†å‰²
                chunks = self._split_text(content)
                
                for chunk in chunks:
                    if len(chunk.strip()) < 10:  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                        continue
                        
                    language = self.processor.detect_language(chunk)
                    
                    texts.append(chunk)
                    labels.append(filename)
                    languages.append(language)
                    
            except Exception as e:
                print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {filepath} - {e}")
                continue
        
        return texts, labels, languages
    
    def _split_text(self, text: str, overlap: int = 50) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªé•·ã•ã«åˆ†å‰²ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä»˜ãï¼‰"""
        if len(text) <= self.max_length:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.max_length
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # æ–‡å¢ƒç•Œã§åˆ†å‰²ã‚’è©¦è¡Œ
            chunk = text[start:end]
            last_period = chunk.rfind('ã€‚')
            last_newline = chunk.rfind('\n')
            last_space = chunk.rfind(' ')
            
            split_pos = max(last_period, last_newline, last_space)
            
            if split_pos > start + self.max_length // 2:
                end = start + split_pos + 1
            
            chunks.append(text[start:end])
            start = end - overlap  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        
        return chunks
    
    def _compute_sample_weights(self) -> List[float]:
        """è¨€èªãƒãƒ©ãƒ³ã‚¹èª¿æ•´ç”¨ã®é‡ã¿è¨ˆç®—"""
        language_counts = Counter(self.languages)
        total_samples = len(self.languages)
        
        # å„è¨€èªã®é‡ã¿ã‚’è¨ˆç®—ï¼ˆé€†é »åº¦é‡ã¿ä»˜ã‘ï¼‰
        language_weights = {}
        for lang, count in language_counts.items():
            language_weights[lang] = total_samples / (len(language_counts) * count)
        
        # ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®é‡ã¿ã‚’è¨ˆç®—
        weights = [language_weights[lang] for lang in self.languages]
        return weights
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        language = self.languages[idx]
        
        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆè¨“ç·´æ™‚ã®ã¿ï¼‰
        if self.augmenter and self.use_augmentation and random.random() < 0.3:
            text = self.augmenter.add_noise(text, self.processor)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        token_ids = self.processor.encode(text)
        
        # é•·ã•èª¿æ•´
        if len(token_ids) > self.max_length:
            token_ids = token_ids[:self.max_length]
        else:
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            token_ids += [0] * (self.max_length - len(token_ids))
        
        # ãƒã‚¹ã‚­ãƒ³ã‚°ï¼ˆç¢ºç‡çš„ã«é©ç”¨ï¼‰
        masked_ids = token_ids.copy()
        if self.augmenter and random.random() < 0.2:  # 20%ã®ç¢ºç‡
            masked_ids, _ = self.augmenter.mask_tokens(token_ids, self.processor.special_tokens['<mask>'])
        
        return {
            'input_ids': torch.tensor(token_ids, dtype=torch.long),
            'labels': torch.tensor(token_ids, dtype=torch.long),  # è‡ªå·±æ•™å¸«å­¦ç¿’
            'language': language,
            'length': len([t for t in token_ids if t != 0])  # å®Ÿéš›ã®é•·ã•
        }


def create_balanced_dataloader(dataset: ImprovedTextDataset, batch_size: int = 16, 
                             use_weighted_sampling: bool = True) -> DataLoader:
    """ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
    
    if use_weighted_sampling and dataset.sample_weights:
        # é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        sampler = WeightedRandomSampler(
            weights=dataset.sample_weights,
            num_samples=len(dataset),
            replacement=True
        )
        shuffle = False
    else:
        sampler = None
        shuffle = True
    
    # å‹•çš„ãƒãƒƒãƒå‡¦ç†ç”¨ã®collateé–¢æ•°
    def collate_fn(batch):
        # ãƒãƒƒãƒãŒè¾æ›¸ã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        if not isinstance(batch, list) or not all(isinstance(item, dict) for item in batch):
            raise TypeError(f"ãƒãƒƒãƒã¯è¾æ›¸ã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ç¾åœ¨ã®å‹: {type(batch)}")
        
        # é•·ã•ã§ã‚½ãƒ¼ãƒˆï¼ˆåŠ¹ç‡çš„ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ãŸã‚ï¼‰
        batch = sorted(batch, key=lambda x: x['length'], reverse=True)
        
        # ãƒãƒƒãƒå†…ã®æœ€å¤§é•·ã«åˆã‚ã›ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°èª¿æ•´
        max_len = max(item['length'] for item in batch)
        max_len = min(max_len, dataset.max_length)  # æœ€å¤§é•·åˆ¶é™
        
        input_ids = []
        labels = []
        languages = []
        lengths = []
        
        for item in batch:
            # å¿…è¦ã«å¿œã˜ã¦ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
            input_id = item['input_ids'][:max_len]
            label = item['labels'][:max_len]
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            if len(input_id) < max_len:
                padding_size = max_len - len(input_id)
                input_id = torch.cat([input_id, torch.zeros(padding_size, dtype=torch.long)])
                label = torch.cat([label, torch.full((padding_size,), -100, dtype=torch.long)])
            
            input_ids.append(input_id)
            labels.append(label)
            languages.append(item['language'])
            lengths.append(item['length'])
        
        return {
            'input_ids': torch.stack(input_ids),
            'labels': torch.stack(labels),
            'languages': languages,
            'lengths': torch.tensor(lengths)
        }
    
    return DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=shuffle,
        sampler=sampler,
        collate_fn=collate_fn,
        num_workers=0,  # Windowsäº’æ›æ€§ã®ãŸã‚0ã«è¨­å®š
        pin_memory=False  # CPUä½¿ç”¨æ™‚ã¯Falseã«è¨­å®š
    )


def create_sample_data(data_dir: str = "sample_data"):
    """å¤§å¹…ã«å¼·åŒ–ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    os.makedirs(data_dir, exist_ok=True)
    
    # å¤§å¹…ã«å¢—å¼·ã•ã‚ŒãŸæ—¥æœ¬èªã‚µãƒ³ãƒ—ãƒ«ï¼ˆæŠ€è¡“ãƒ»å­¦è¡“ãƒ»æ—¥å¸¸ä¼šè©±ãªã©å¤šæ§˜ãªã‚¸ãƒ£ãƒ³ãƒ«ï¼‰
    japanese_samples = [
        # æ©Ÿæ¢°å­¦ç¿’ãƒ»AIé–¢é€£
        "æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦çµ±è¨ˆçš„ãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€äºˆæ¸¬ã‚„åˆ†é¡ã‚’è¡Œã„ã¾ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¤šå±¤ã«ç©ã¿é‡ã­ãŸæ‰‹æ³•ã§ã€ç”»åƒèªè­˜ã‚„è‡ªç„¶è¨€èªå‡¦ç†ã«å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒä»£è¡¨çš„ã§ã™ã€‚",
        "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã„ã¦é©æ–°çš„ãªé€²æ­©ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã«ã‚ˆã‚Šé•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«å­¦ç¿’ã§ãã¾ã™ã€‚",
        "Mambaãƒ¢ãƒ‡ãƒ«ã¯çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡¦ç†ã«åŠ¹ç‡çš„ã§ã™ã€‚è¨ˆç®—è¤‡é›‘åº¦ãŒç·šå½¢ã§ã‚ã‚‹ã“ã¨ãŒå¤§ããªç‰¹å¾´ã§ã™ã€‚",
        "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®åé›†ã€å‰å‡¦ç†ã€åˆ†æã€å¯è¦–åŒ–ãŒé‡è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚çµ±è¨ˆçš„æ‰‹æ³•ã¨æ©Ÿæ¢°å­¦ç¿’ã‚’çµ„ã¿åˆã‚ã›ã¦çŸ¥è¦‹ã‚’æŠ½å‡ºã—ã¾ã™ã€‚",
        "å¼·åŒ–å­¦ç¿’ã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚å ±é…¬ä¿¡å·ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãªæ”¿ç­–ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒç›®æ¨™ã§ã™ã€‚",
        "ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹é‡è¦ãªæ‰‹æ³•ã®ä¸€ã¤ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã®éƒ¨åˆ†ã«åˆ†å‰²ã—ã¦è¨“ç·´ã¨æ¤œè¨¼ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚",
        "ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’é˜²ããŸã‚ã«ã€æ­£å‰‡åŒ–ã‚„ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆãªã©ã®æŠ€è¡“ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚",
        
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ»æŠ€è¡“é–¢é€£
        "Pythonã¯æ©Ÿæ¢°å­¦ç¿’åˆ†é‡ã§æœ€ã‚‚äººæ°—ã®ã‚ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚NumPyã€pandasã€scikit-learnãªã©ã®è±Šå¯Œãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã™ã€‚",
        "Git ã¯åˆ†æ•£å‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã«ãŠã„ã¦æ¬ ã‹ã›ãªã„ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ãƒãƒ¼ãƒ é–‹ç™ºã§ã¯å¿…é ˆã®æŠ€è¡“ã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã§ã¯æ­£è¦åŒ–ãŒé‡è¦ãªæ¦‚å¿µã§ã™ã€‚ç¬¬ä¸€æ­£è¦å½¢ã€ç¬¬äºŒæ­£è¦å½¢ã€ç¬¬ä¸‰æ­£è¦å½¢ãªã©ã®æ®µéšçš„ãªæ­£è¦åŒ–ã‚’è¡Œã„ã¾ã™ã€‚",
        "Webé–‹ç™ºã«ãŠã„ã¦RESTful APIã¯æ¨™æº–çš„ãªè¨­è¨ˆæ‰‹æ³•ã§ã™ã€‚HTTPãƒ¡ã‚½ãƒƒãƒ‰ã‚’é©åˆ‡ã«ä½¿ã„åˆ†ã‘ã¦ãƒªã‚½ãƒ¼ã‚¹æŒ‡å‘ã®è¨­è¨ˆã‚’è¡Œã„ã¾ã™ã€‚",
        
        # æ—¥æœ¬æ–‡åŒ–ãƒ»æ­´å²
        "æ—¥æœ¬ã®å››å­£ã¯ä¸–ç•Œçš„ã«ã‚‚ç¾ã—ã„ã¨ã•ã‚Œã€æ˜¥ã®æ¡œã€å¤ã®ç·‘ã€ç§‹ã®ç´…è‘‰ã€å†¬ã®é›ªæ™¯è‰²ãŒãã‚Œãã‚Œç‹¬ç‰¹ã®é­…åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã€‚",
        "èŒ¶é“ã¯æ—¥æœ¬ã®ä¼çµ±æ–‡åŒ–ã®ä¸€ã¤ã§ã€ãŠã‚‚ã¦ãªã—ã®å¿ƒã¨ç²¾ç¥çš„ãªä¿®é¤Šã‚’é‡è¦–ã—ã¾ã™ã€‚åƒåˆ©ä¼‘ãŒå¤§æˆã—ãŸä¾˜ã³èŒ¶ãŒç‰¹ã«æœ‰åã§ã™ã€‚",
        "æ±Ÿæˆ¸æ™‚ä»£ã®é–å›½æ”¿ç­–ã¯æ—¥æœ¬ç‹¬è‡ªã®æ–‡åŒ–ç™ºå±•ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚æµ®ä¸–çµµã‚„æ­Œèˆä¼ãªã©ã®åº¶æ°‘æ–‡åŒ–ãŒèŠ±é–‹ã„ãŸæ™‚ä»£ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚",
        "æ˜æ²»ç¶­æ–°ã¯æ—¥æœ¬ã®è¿‘ä»£åŒ–ã®å‡ºç™ºç‚¹ã¨ãªã‚Šã¾ã—ãŸã€‚å°å»ºåˆ¶åº¦ã‹ã‚‰è¿‘ä»£å›½å®¶ã¸ã®è»¢æ›ã¯ä¸–ç•Œå²çš„ã«ã‚‚æ³¨ç›®ã•ã‚Œã‚‹å¤‰åŒ–ã§ã—ãŸã€‚",
        
        # ç§‘å­¦ãƒ»è‡ªç„¶
        "é‡å­åŠ›å­¦ã¯ç‰©ç†å­¦ã®åŸºæœ¬ç†è«–ã®ä¸€ã¤ã§ã€åŸå­ã‚„é›»å­ãªã©ã®ãƒŸã‚¯ãƒ­ãªä¸–ç•Œã®ç¾è±¡ã‚’è¨˜è¿°ã—ã¾ã™ã€‚ä¸ç¢ºå®šæ€§åŸç†ãŒé‡è¦ãªæ¦‚å¿µã§ã™ã€‚",
        "åœ°çƒæ¸©æš–åŒ–ã¯ç¾ä»£ç¤¾ä¼šãŒç›´é¢ã™ã‚‹é‡è¦ãªèª²é¡Œã§ã™ã€‚äºŒé…¸åŒ–ç‚­ç´ ãªã©ã®æ¸©å®¤åŠ¹æœã‚¬ã‚¹ã®å‰Šæ¸›ãŒæ€¥å‹™ã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "ç”Ÿç‰©å¤šæ§˜æ€§ã®ä¿å…¨ã¯åœ°çƒç’°å¢ƒã®æŒç¶šå¯èƒ½æ€§ã«ã¨ã£ã¦ä¸å¯æ¬ ã§ã™ã€‚ç”Ÿæ…‹ç³»ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚",
        "å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æ™®åŠãŒä¸–ç•Œçš„ã«é€²ã‚“ã§ã„ã¾ã™ã€‚å¤ªé™½å…‰ã€é¢¨åŠ›ã€æ°´åŠ›ãªã©ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒæ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        
        # æ—¥å¸¸ãƒ»ç¤¾ä¼š
        "åƒãæ–¹æ”¹é©ã«ã‚ˆã‚Šã€ãƒ†ãƒ¬ãƒ¯ãƒ¼ã‚¯ã‚„ãƒ•ãƒ¬ãƒƒã‚¯ã‚¹ã‚¿ã‚¤ãƒ åˆ¶åº¦ãŒæ™®åŠã—ã¦ã„ã¾ã™ã€‚ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã®é‡è¦æ€§ãŒèªè­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "é«˜é½¢åŒ–ç¤¾ä¼šã«ãŠã„ã¦ã€åŒ»ç™‚ãƒ»ä»‹è­·åˆ†é‡ã§ã®äººæä¸è¶³ãŒæ·±åˆ»ãªå•é¡Œã¨ãªã£ã¦ã„ã¾ã™ã€‚ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®æ´»ç”¨ã«ã‚ˆã‚‹è§£æ±ºç­–ãŒæ¨¡ç´¢ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "æ•™è‚²ç¾å ´ã§ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ãŒæ€¥é€Ÿã«é€²ã‚“ã§ã„ã¾ã™ã€‚ä¸€äººä¸€å°ã®ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆç«¯æœ«é…å¸ƒã«ã‚ˆã‚Šå­¦ç¿’ç’°å¢ƒãŒå¤§ããå¤‰åŒ–ã—ã¦ã„ã¾ã™ã€‚",
        "éƒ½å¸‚éƒ¨ã¸ã®äººå£é›†ä¸­ãŒé€²ã‚€ä¸­ã€åœ°æ–¹å‰µç”Ÿã®å–ã‚Šçµ„ã¿ãŒé‡è¦è¦–ã•ã‚Œã¦ã„ã¾ã™ã€‚ç§»ä½ä¿ƒé€²ã‚„èµ·æ¥­æ”¯æ´ãªã©ã®æ–½ç­–ãŒå±•é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚"
    ]
    
    # å¤§å¹…ã«å¢—å¼·ã•ã‚ŒãŸè‹±èªã‚µãƒ³ãƒ—ãƒ«ï¼ˆæŠ€è¡“ãƒ»å­¦è¡“ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ»æ—¥å¸¸ãªã©å¤šæ§˜ãªã‚¸ãƒ£ãƒ³ãƒ«ï¼‰
    english_samples = [
        # Machine Learning & AI
        "Machine learning is a subset of artificial intelligence that focuses on algorithms and statistical models to enable computers to improve their performance on specific tasks through experience.",
        "Deep learning models have achieved remarkable success in computer vision and natural language processing tasks by utilizing multiple layers of neural networks to learn hierarchical representations.",
        "The transformer architecture revolutionized the field of natural language processing with attention mechanisms that allow models to focus on relevant parts of input sequences simultaneously.",
        "State space models like Mamba offer efficient alternatives to transformers for long sequence modeling with linear computational complexity and improved memory efficiency.",
        "Data preprocessing is a crucial step that significantly impacts the performance of machine learning models, including cleaning, normalization, and feature extraction processes.",
        "Regularization techniques help prevent overfitting and improve model generalization by adding penalty terms to the loss function or using dropout mechanisms.",
        "Cross-validation provides robust estimates of model performance across different data splits, helping to assess how well models generalize to unseen data.",
        "Feature engineering involves creating meaningful input variables that improve model accuracy through domain knowledge and statistical analysis of the data.",
        
        # Technology & Programming
        "Python has become the dominant programming language in data science and machine learning due to its simplicity, readability, and extensive ecosystem of libraries.",
        "Version control systems like Git enable collaborative software development by tracking changes to code and managing different versions of projects efficiently.",
        "Cloud computing platforms provide scalable infrastructure for machine learning workloads, allowing researchers and companies to access powerful computing resources on demand.",
        "DevOps practices integrate software development and operations to enable faster deployment cycles and more reliable systems through automation and monitoring.",
        "Cybersecurity has become increasingly important as digital transformation accelerates, requiring robust defenses against evolving threats and vulnerabilities.",
        "Blockchain technology offers decentralized solutions for various applications beyond cryptocurrency, including supply chain management and digital identity verification.",
        
        # Science & Research
        "Quantum computing represents a paradigm shift in computational power, utilizing quantum mechanical phenomena to solve certain problems exponentially faster than classical computers.",
        "Climate change research relies heavily on large-scale data analysis and predictive modeling to understand complex environmental systems and project future scenarios.",
        "Biotechnology advances are revolutionizing medicine through personalized treatments, gene therapy, and precision medicine approaches tailored to individual genetic profiles.",
        "Space exploration has entered a new era with private companies collaborating with government agencies to develop reusable rockets and establish permanent human presence beyond Earth.",
        "Renewable energy technologies are becoming increasingly cost-competitive with fossil fuels, driving global transitions toward sustainable energy systems.",
        
        # Business & Economics
        "Digital transformation is reshaping traditional business models across industries, requiring organizations to adapt to new technologies and changing customer expectations.",
        "E-commerce platforms have fundamentally changed retail landscapes, offering consumers unprecedented convenience while creating new challenges for traditional brick-and-mortar stores.",
        "Artificial intelligence is being integrated into business processes to automate routine tasks, enhance decision-making, and provide personalized customer experiences.",
        "Supply chain optimization has become critical for global businesses, especially in the wake of recent disruptions that highlighted the importance of resilience and flexibility.",
        "Financial technology innovations are democratizing access to financial services through mobile banking, peer-to-peer lending, and digital payment solutions.",
        
        # Society & Culture
        "Social media platforms have transformed how people communicate, share information, and form communities, while also raising concerns about privacy and misinformation.",
        "Remote work has become mainstream following global shifts in workplace culture, offering flexibility while presenting new challenges for collaboration and management.",
        "Educational technology is revolutionizing learning experiences through personalized curricula, virtual classrooms, and adaptive learning systems that cater to individual needs.",
        "Urban planning increasingly incorporates smart city technologies to improve efficiency, sustainability, and quality of life for growing metropolitan populations.",
        "Healthcare systems worldwide are adopting digital health solutions, including telemedicine, wearable devices, and AI-assisted diagnostics to improve patient outcomes."
    ]
    
    # å¤§å¹…ã«å¢—å¼·ã•ã‚ŒãŸPythonã‚µãƒ³ãƒ—ãƒ«ï¼ˆæ©Ÿæ¢°å­¦ç¿’ã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€Webé–‹ç™ºãªã©ï¼‰
    python_samples = [
        '''
# Advanced PyTorch Model with Attention
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            
        attention = F.softmax(scores, dim=-1)
        attention = self.dropout(attention)
        
        output = torch.matmul(attention, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        return self.w_o(output)
''',
        '''
# Data Science Pipeline with Pandas and Scikit-learn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class MLPipeline:
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.label_encoder = LabelEncoder()
        
    def preprocess_data(self, df, target_column):
        # Handle missing values
        df_clean = df.dropna()
        
        # Separate features and target
        X = df_clean.drop(target_column, axis=1)
        y = df_clean[target_column]
        
        # Encode categorical variables
        categorical_columns = X.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            X[col] = self.label_encoder.fit_transform(X[col])
            
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        return X_scaled, y
        
    def train_and_evaluate(self, X, y):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train model
        self.model.fit(X_train, y_train)
        
        # Make predictions
        y_pred = self.model.predict(X_test)
        
        # Evaluate
        print("Classification Report:")
        print(classification_report(y_test, y_pred))
        
        # Cross-validation
        cv_scores = cross_val_score(self.model, X, y, cv=5)
        print(f"Cross-validation scores: {cv_scores}")
        print(f"Mean CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
        
        return y_test, y_pred
''',
        '''
# Advanced Web API with FastAPI
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Optional
import uvicorn
from datetime import datetime
import sqlite3
import hashlib

app = FastAPI(title="ML Model API", version="1.0.0")

class PredictionRequest(BaseModel):
    features: List[float]
    model_version: Optional[str] = "latest"

class PredictionResponse(BaseModel):
    prediction: float
    confidence: float
    model_version: str
    timestamp: datetime

class MLModelService:
    def __init__(self):
        self.models = {}
        self.load_models()
        
    def load_models(self):
        # Load pre-trained models
        pass
        
    def predict(self, features: List[float], model_version: str = "latest"):
        if model_version not in self.models:
            raise HTTPException(status_code=404, detail="Model version not found")
            
        # Make prediction
        prediction = sum(features) / len(features)  # Dummy prediction
        confidence = 0.85
        
        return prediction, confidence

ml_service = MLModelService()

@app.get("/")
async def root():
    return {"message": "ML Model API is running"}

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    try:
        prediction, confidence = ml_service.predict(request.features, request.model_version)
        
        return PredictionResponse(
            prediction=prediction,
            confidence=confidence,
            model_version=request.model_version,
            timestamp=datetime.now()
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def list_models():
    return {"available_models": list(ml_service.models.keys())}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
''',
        '''
# Advanced Data Processing and Visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

class DataAnalyzer:
    def __init__(self, data_path):
        self.df = pd.read_csv(data_path)
        self.numerical_columns = self.df.select_dtypes(include=[np.number]).columns
        self.categorical_columns = self.df.select_dtypes(include=['object']).columns
        
    def exploratory_analysis(self):
        print("Dataset Shape:", self.df.shape)
        print("\\nMissing Values:")
        print(self.df.isnull().sum())
        print("\\nNumerical Statistics:")
        print(self.df[self.numerical_columns].describe())
        
    def create_visualizations(self):
        # Create subplots
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Distribution Plot', 'Correlation Heatmap', 
                          'Box Plot', 'Scatter Plot'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # Distribution plot
        if len(self.numerical_columns) > 0:
            col = self.numerical_columns[0]
            fig.add_trace(go.Histogram(x=self.df[col], name=col), row=1, col=1)
            
        # Correlation heatmap data
        corr_matrix = self.df[self.numerical_columns].corr()
        
        # Box plot
        if len(self.numerical_columns) > 1:
            fig.add_trace(go.Box(y=self.df[self.numerical_columns[1]], 
                               name=self.numerical_columns[1]), row=2, col=1)
            
        # Scatter plot
        if len(self.numerical_columns) >= 2:
            fig.add_trace(go.Scatter(x=self.df[self.numerical_columns[0]], 
                                   y=self.df[self.numerical_columns[1]],
                                   mode='markers', name='Scatter'), row=2, col=2)
        
        fig.update_layout(height=800, showlegend=False, title_text="Data Analysis Dashboard")
        fig.show()
        
    def statistical_tests(self):
        results = {}
        
        for col in self.numerical_columns:
            # Normality test
            stat, p_value = stats.normaltest(self.df[col].dropna())
            results[f'{col}_normality'] = {'statistic': stat, 'p_value': p_value}
            
        return results
''',
        '''
# Advanced Class Design with Decorators and Context Managers
import functools
import time
import logging
from contextlib import contextmanager
from typing import Any, Callable, Dict, List

def timing_decorator(func: Callable) -> Callable:
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__} executed in {end_time - start_time:.4f} seconds")
        return result
    return wrapper

def retry_decorator(max_attempts: int = 3, delay: float = 1.0):
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise e
                    print(f"Attempt {attempt + 1} failed: {e}. Retrying in {delay} seconds...")
                    time.sleep(delay)
        return wrapper
    return decorator

class DataProcessor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
    @contextmanager
    def processing_context(self, operation_name: str):
        self.logger.info(f"Starting {operation_name}")
        try:
            yield
        except Exception as e:
            self.logger.error(f"Error in {operation_name}: {e}")
            raise
        finally:
            self.logger.info(f"Completed {operation_name}")
            
    @timing_decorator
    @retry_decorator(max_attempts=3)
    def process_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        with self.processing_context("data_processing"):
            processed_data = []
            
            for item in data:
                processed_item = self._transform_item(item)
                if self._validate_item(processed_item):
                    processed_data.append(processed_item)
                    
            return processed_data
            
    def _transform_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        # Apply transformations based on config
        transformed = item.copy()
        
        for key, value in item.items():
            if key in self.config.get('normalize_fields', []):
                transformed[key] = self._normalize_value(value)
                
        return transformed
        
    def _normalize_value(self, value: Any) -> Any:
        if isinstance(value, str):
            return value.lower().strip()
        elif isinstance(value, (int, float)):
            return (value - self.config.get('mean', 0)) / self.config.get('std', 1)
        return value
        
    def _validate_item(self, item: Dict[str, Any]) -> bool:
        required_fields = self.config.get('required_fields', [])
        return all(field in item and item[field] is not None for field in required_fields)
'''
    ]
    
    # æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªï¼šæŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    technical_docs = [
        "API Documentation: The RESTful API follows standard HTTP methods and status codes. GET requests retrieve data, POST creates new resources, PUT updates existing resources, and DELETE removes resources.",
        "System Architecture: The microservices architecture consists of loosely coupled services that communicate through well-defined APIs. Each service has its own database and can be deployed independently.",
        "Performance Optimization: Database query optimization involves proper indexing, query plan analysis, and avoiding N+1 problems. Use connection pooling and caching strategies for better performance.",
        "Security Guidelines: Implement authentication using JWT tokens, authorize access based on user roles, and sanitize all user inputs to prevent SQL injection and XSS attacks.",
        "Deployment Process: Use containerization with Docker for consistent environments. Implement CI/CD pipelines with automated testing, staging deployments, and blue-green production releases.",
        "Monitoring and Logging: Set up comprehensive logging with structured formats. Monitor system metrics, application performance, and business KPIs using tools like Prometheus and Grafana."
    ]
    
    # æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªï¼šå­¦è¡“ãƒ»ç ”ç©¶é¢¨ãƒ†ã‚­ã‚¹ãƒˆ
    academic_samples = [
        "Abstract: This study investigates the application of deep learning techniques to natural language processing tasks. We propose a novel architecture that combines attention mechanisms with recurrent neural networks.",
        "Methodology: The experimental design includes a control group and treatment group. Data collection follows ethical guidelines with informed consent from all participants.",
        "Results: Statistical analysis reveals a significant correlation between variables X and Y (p < 0.05). The confidence interval provides robust evidence for the proposed hypothesis.",
        "Discussion: The findings suggest that machine learning models can effectively predict outcomes with 85% accuracy. However, limitations include dataset size and potential bias in the training data.",
        "Conclusion: Future research should explore larger datasets and more diverse populations. The proposed framework demonstrates promising results for real-world applications."
    ]
    
    # å¤šè¨€èªæ··åˆãƒ†ã‚­ã‚¹ãƒˆ
    multilingual_samples = [
        "æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹ cross-validation ã¯é‡è¦ãªæŠ€è¡“ã§ã™ã€‚This technique helps evaluate model performance across different data splits.",
        "Deep learning models like transformers have revolutionized NLP. è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§ã¯ transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå¤§ããªå¤‰é©ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚",
        "Python libraries such as pandas and numpy are essential for data science. ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã§ã¯ pandas ã‚„ numpy ã¨ã„ã£ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…é ˆã§ã™ã€‚"
    ]
    
    # ã‚ˆã‚Šå¤šæ§˜ãªãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    file_data = {
        "japanese.txt": japanese_samples,
        "english.txt": english_samples,
        "sample_code.py": python_samples,
        "technical_docs.txt": technical_docs,
        "academic_papers.txt": academic_samples,
        "multilingual.txt": multilingual_samples
    }
    
    total_texts = 0
    for filename, samples in file_data.items():
        filepath = os.path.join(data_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write("\n\n".join(samples))
        total_texts += len(samples)
        print(f"ğŸ“„ {filename}: {len(samples)} samples")
    
    print(f"ğŸ“ å¤§å¹…ã«å¼·åŒ–ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸ: {data_dir}")
    print(f"ğŸ“Š ç·ãƒ†ã‚­ã‚¹ãƒˆæ•°: {total_texts} (å‰å›ã®9ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¤§å¹…å¢—åŠ )")