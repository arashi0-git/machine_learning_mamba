#!/usr/bin/env python3
"""
Hybrid Mamba Model ã®åŸºæœ¬ãƒ†ã‚¹ãƒˆ
"""
import torch
import sys
import traceback

try:
    from hybrid_mamba_model import HybridMambaModel
    print("âœ“ HybridMambaModelã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
    
    # åŸºæœ¬çš„ãªè¨­å®š
    vocab_size = 1000
    d_model = 128
    n_layers = 2
    n_heads = 4
    num_experts = 2
    max_seq_len = 64
    
    print(f"ãƒ¢ãƒ‡ãƒ«è¨­å®š:")
    print(f"  èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
    print(f"  ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ: {d_model}")
    print(f"  ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {n_layers}")
    print(f"  ãƒ˜ãƒƒãƒ‰æ•°: {n_heads}")
    print(f"  ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°: {num_experts}")
    print(f"  æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {max_seq_len}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    model = HybridMambaModel(
        vocab_size=vocab_size,
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        num_experts=num_experts,
        max_seq_len=max_seq_len,
        dropout=0.1
    )
    print("âœ“ HybridMambaModelã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–æˆåŠŸ")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
    model_info = model.get_model_info()
    print(f"\nãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
    print(f"  ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_info['total_parameters']:,}")
    print(f"  å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_info['trainable_parameters']:,}")
    print(f"  ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_info['model_type']}")
    
    # ç°¡å˜ãªå‰å‘ãè¨ˆç®—ãƒ†ã‚¹ãƒˆ
    batch_size = 2
    seq_len = 32
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    print(f"\nå‰å‘ãè¨ˆç®—ãƒ†ã‚¹ãƒˆ:")
    print(f"  å…¥åŠ›å½¢çŠ¶: {input_ids.shape}")
    
    # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids, labels=labels)
        
    print(f"âœ“ å‰å‘ãè¨ˆç®—æˆåŠŸ")
    print(f"  å‡ºåŠ›logitså½¢çŠ¶: {outputs['logits'].shape}")
    print(f"  æå¤±: {outputs['loss']:.4f}")
    print(f"  è² è·ãƒãƒ©ãƒ³ã‚¹æå¤±: {outputs['balance_loss']:.4f}")
    
    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print(f"\nç”Ÿæˆãƒ†ã‚¹ãƒˆ:")
    test_input = torch.randint(0, vocab_size, (1, 10))
    generated = model.generate(test_input, max_length=20, temperature=1.0)
    print(f"  å…¥åŠ›é•·: {test_input.size(1)}")
    print(f"  ç”Ÿæˆé•·: {generated.size(1)}")
    print(f"âœ“ ç”Ÿæˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
    
    print(f"\nğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
    print(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ« (Mamba Ã— MoE Ã— Transformer) ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    print(f"è©³ç´°:")
    traceback.print_exc()
    sys.exit(1)