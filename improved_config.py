#!/usr/bin/env python3
"""
ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®æ”¹å–„ã•ã‚ŒãŸHybrid Mambaè¨­å®š
"""

import torch
import torch.nn as nn
from dataclasses import dataclass
from typing import Dict, Any


@dataclass
class ImprovedModelConfig:
    """æ”¹å–„ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«è¨­å®š"""
    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
    d_model: int = 512  # å¤§å¹…å¢—åŠ 
    n_layers: int = 8   # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°å¢—åŠ 
    d_state: int = 32   # çŠ¶æ…‹æ¬¡å…ƒå¢—åŠ 
    n_heads: int = 16   # ãƒ˜ãƒƒãƒ‰æ•°å¢—åŠ 
    num_experts: int = 8  # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°å¢—åŠ 
    expert_capacity: float = 1.25  # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆå®¹é‡
    
    # MoEè¨­å®š
    top_k: int = 2
    balance_loss_weight: float = 0.01  # ãƒãƒ©ãƒ³ã‚¹æå¤±ã®é‡ã¿
    expert_dropout: float = 0.1
    expert_activation: str = 'swiglu'  # ã‚ˆã‚ŠåŠ¹æœçš„ãªæ´»æ€§åŒ–
    
    # Attentionè¨­å®š
    use_rope: bool = True
    use_flash_attention: bool = True
    attention_dropout: float = 0.1
    
    # æ­£å‰‡åŒ–
    dropout: float = 0.15
    weight_decay: float = 0.1
    label_smoothing: float = 0.1
    
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å‡¦ç†
    max_seq_len: int = 512  # é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å¯¾å¿œ
    
    # èªå½™ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    vocab_size: int = 32000  # ã‚ˆã‚Šå¤§ããªèªå½™ã‚µã‚¤ã‚º
    pad_token_id: int = 0
    eos_token_id: int = 1
    bos_token_id: int = 2


@dataclass 
class ImprovedTrainingConfig:
    """æ”¹å–„ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š"""
    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
    learning_rate: float = 5e-4
    min_learning_rate: float = 1e-6
    warmup_steps: int = 1000
    max_steps: int = 10000
    
    # ãƒãƒƒãƒå‡¦ç†
    batch_size: int = 16  # å¢—åŠ 
    gradient_accumulation_steps: int = 4
    max_grad_norm: float = 1.0
    
    # æœ€é©åŒ–
    optimizer: str = 'adamw'
    beta1: float = 0.9
    beta2: float = 0.95
    eps: float = 1e-8
    
    # è©•ä¾¡ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    eval_steps: int = 500
    save_steps: int = 1000
    logging_steps: int = 100
    
    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
    use_data_augmentation: bool = True
    max_position_embeddings: int = 512


def get_improved_model_args() -> Dict[str, Any]:
    """æ”¹å–„ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å¼•æ•°ã‚’å–å¾—"""
    config = ImprovedModelConfig()
    return {
        'vocab_size': config.vocab_size,
        'd_model': config.d_model, 
        'n_layers': config.n_layers,
        'd_state': config.d_state,
        'n_heads': config.n_heads,
        'num_experts': config.num_experts,
        'max_seq_len': config.max_seq_len,
        'dropout': config.dropout,
        'use_gradient_checkpointing': True,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
    }


def get_improved_training_args() -> Dict[str, Any]:
    """æ”¹å–„ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’å–å¾—"""
    config = ImprovedTrainingConfig()
    return {
        'learning_rate': config.learning_rate,
        'batch_size': config.batch_size,
        'max_steps': config.max_steps,
        'warmup_steps': config.warmup_steps,
        'weight_decay': 0.1,
        'gradient_accumulation_steps': config.gradient_accumulation_steps,
        'max_grad_norm': config.max_grad_norm,
        'eval_steps': config.eval_steps,
        'save_steps': config.save_steps,
        'logging_steps': config.logging_steps,
    }


class ImprovedLearningRateScheduler:
    """æ”¹å–„ã•ã‚ŒãŸå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""
    def __init__(self, optimizer, warmup_steps: int, max_steps: int, 
                 min_lr_ratio: float = 0.1):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.max_steps = max_steps
        self.min_lr_ratio = min_lr_ratio
        self.initial_lr = optimizer.param_groups[0]['lr']
        self.step_count = 0
        
    def step(self):
        self.step_count += 1
        lr = self.get_lr()
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
            
    def get_lr(self):
        if self.step_count < self.warmup_steps:
            # WarmupæœŸé–“ï¼šç·šå½¢å¢—åŠ 
            return self.initial_lr * (self.step_count / self.warmup_steps)
        else:
            # ã‚³ã‚µã‚¤ãƒ³ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°
            progress = (self.step_count - self.warmup_steps) / (self.max_steps - self.warmup_steps)
            progress = min(progress, 1.0)
            cosine_decay = 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)))
            return self.initial_lr * (self.min_lr_ratio + (1 - self.min_lr_ratio) * cosine_decay)


def print_model_config():
    """ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’è¡¨ç¤º"""
    model_config = ImprovedModelConfig()
    training_config = ImprovedTrainingConfig()
    
    print("=" * 60)
    print("æ”¹å–„ã•ã‚ŒãŸHybrid Mambaãƒ¢ãƒ‡ãƒ«è¨­å®š")
    print("=" * 60)
    print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:")
    print(f"  - ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ: {model_config.d_model}")
    print(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {model_config.n_layers}")
    print(f"  - çŠ¶æ…‹æ¬¡å…ƒ: {model_config.d_state}")
    print(f"  - ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰: {model_config.n_heads}")
    print(f"  - MoEã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°: {model_config.num_experts}")
    print(f"  - èªå½™ã‚µã‚¤ã‚º: {model_config.vocab_size}")
    print(f"  - æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {model_config.max_seq_len}")
    
    print("\nğŸ¯ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š:")
    print(f"  - å­¦ç¿’ç‡: {training_config.learning_rate}")
    print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_config.batch_size}")
    print(f"  - å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—: {training_config.gradient_accumulation_steps}")
    print(f"  - æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {training_config.max_steps}")
    print(f"  - ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—: {training_config.warmup_steps}")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æ¨å®š
    estimated_params = estimate_parameters(model_config)
    print(f"\nğŸ’¾ æ¨å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {estimated_params:,}")
    print("=" * 60)


def estimate_parameters(config: ImprovedModelConfig) -> int:
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æ¨å®š"""
    vocab_params = config.vocab_size * config.d_model * 2  # embedding + lm_head
    
    # Mambaãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    mamba_params_per_layer = (
        config.d_model * config.d_model * 6 +  # æŠ•å½±å±¤
        config.d_model * config.d_state * 3 +  # çŠ¶æ…‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        config.d_state * 2  # A, D ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    )
    
    # Attentionã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    attn_params_per_layer = config.d_model * config.d_model * 4  # Q, K, V, O
    
    # MoEã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    expert_params_per_layer = config.num_experts * config.d_model * config.d_model * 3
    gate_params_per_layer = config.d_model * config.num_experts
    
    layer_params = (mamba_params_per_layer + attn_params_per_layer + 
                   expert_params_per_layer + gate_params_per_layer)
    
    total_params = vocab_params + (layer_params * config.n_layers)
    return total_params


if __name__ == "__main__":
    print_model_config() 