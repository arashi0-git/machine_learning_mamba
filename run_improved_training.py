#!/usr/bin/env python3
"""
ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®æ”¹è‰¯ã•ã‚ŒãŸMambaãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import time
import argparse
import torch
import numpy as np
import random
from typing import Dict, Any
import json

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from hybrid_mamba_model import HybridMambaModel
from improved_trainer import ImprovedMambaTrainer
from improved_config import (
    get_improved_model_args, 
    get_improved_training_args,
    print_model_config
)
from data_loader import (
    ImprovedTextDataset, 
    create_balanced_dataloader, 
    create_sample_data
)


def set_random_seeds(seed: int = 42):
    """å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def setup_device() -> str:
    """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ"""
    if torch.cuda.is_available():
        device = 'cuda'
        print(f"ğŸš€ CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        device = 'cpu'
        print("ğŸ’» CPUã‚’ä½¿ç”¨ã—ã¾ã™")
    
    return device


def print_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
    print("=" * 80)
    print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
    print("=" * 80)
    print(f"Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    print(f"PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}")
    print(f"CUDA åˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
        print(f"GPU ãƒ‡ãƒã‚¤ã‚¹æ•°: {torch.cuda.device_count()}")
    print("=" * 80)


def create_experiment_config(args) -> Dict[str, Any]:
    """å®Ÿé¨“è¨­å®šã‚’ä½œæˆ"""
    model_args = get_improved_model_args()
    training_args = get_improved_training_args()
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ä¸Šæ›¸ã
    if args.d_model:
        model_args['d_model'] = args.d_model
    if args.n_layers:
        model_args['n_layers'] = args.n_layers
    if args.max_seq_len:
        model_args['max_seq_len'] = args.max_seq_len
    if args.num_experts:
        model_args['num_experts'] = args.num_experts
    
    if args.learning_rate:
        training_args['learning_rate'] = args.learning_rate
    if args.batch_size:
        training_args['batch_size'] = args.batch_size
    if args.max_steps:
        training_args['max_steps'] = args.max_steps
    
    return {
        'model': model_args,
        'training': training_args,
        'data_dir': args.data_dir,
        'save_dir': args.save_dir,
        'experiment_name': args.experiment_name,
        'device': args.device
    }


def main():
    parser = argparse.ArgumentParser(description='æ”¹è‰¯ã•ã‚ŒãŸMambaãƒ¢ãƒ‡ãƒ«å­¦ç¿’')
    
    # ãƒ‡ãƒ¼ã‚¿é–¢é€£
    parser.add_argument('--data_dir', type=str, default='sample_data',
                       help='å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--create_sample', action='store_true',
                       help='ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ')
    
    # ãƒ¢ãƒ‡ãƒ«é–¢é€£
    parser.add_argument('--d_model', type=int, default=None,
                       help='ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 512ï¼‰')
    parser.add_argument('--n_layers', type=int, default=None,
                       help='ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8ï¼‰')
    parser.add_argument('--max_seq_len', type=int, default=None,
                       help='æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 512ï¼‰')
    parser.add_argument('--num_experts', type=int, default=None,
                       help='ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8ï¼‰')
    
    # å­¦ç¿’é–¢é€£
    parser.add_argument('--learning_rate', type=float, default=None,
                       help='å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5e-4ï¼‰')
    parser.add_argument('--batch_size', type=int, default=None,
                       help='ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 16ï¼‰')
    parser.add_argument('--max_steps', type=int, default=None,
                       help='æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10000ï¼‰')
    
    # ã‚·ã‚¹ãƒ†ãƒ é–¢é€£
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cpu', 'cuda'], help='ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹')
    parser.add_argument('--save_dir', type=str, default='checkpoints',
                       help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--experiment_name', type=str, default='improved_mamba',
                       help='å®Ÿé¨“å')
    parser.add_argument('--seed', type=int, default=42,
                       help='ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰')
    
    # ãã®ä»–
    parser.add_argument('--test_text', type=str, default='æ©Ÿæ¢°å­¦ç¿’ã¯',
                       help='ç”Ÿæˆãƒ†ã‚¹ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆ')
    parser.add_argument('--no_compile', action='store_true',
                       help='PyTorchã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--no_mixed_precision', action='store_true',
                       help='Mixed Precisionã‚’ç„¡åŠ¹åŒ–')
    
    args = parser.parse_args()
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
    print_system_info()
    
    # ã‚·ãƒ¼ãƒ‰è¨­å®š
    set_random_seeds(args.seed)
    print(f"ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®š: {args.seed}")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    if args.create_sample:
        create_sample_data(args.data_dir)
        if not os.path.exists(args.data_dir) or len(os.listdir(args.data_dir)) == 0:
            print("âŒ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    if args.device == 'auto':
        device = setup_device()
    else:
        device = args.device
    
    # å®Ÿé¨“è¨­å®š
    config = create_experiment_config(args)
    config['device'] = device
    
    # è¨­å®šè¡¨ç¤º
    print_model_config()
    
    print("\n" + "=" * 80)
    print("ğŸ“Š å®Ÿé¨“è¨­å®š")
    print("=" * 80)
    print(f"å®Ÿé¨“å: {config['experiment_name']}")
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {config['data_dir']}")
    print(f"ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {config['save_dir']}")
    print(f"ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print("=" * 80)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        print("\nğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
        dataset = ImprovedTextDataset(
            data_dir=config['data_dir'],
            max_length=config['model']['max_seq_len'],
            use_augmentation=True,
            balance_languages=True
        )
        
        if len(dataset) == 0:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©ºã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        train_dataloader = create_balanced_dataloader(
            dataset,
            batch_size=config['training']['batch_size'],
            use_weighted_sampling=True
        )
        
        # èªå½™ã‚µã‚¤ã‚ºã‚’è¨­å®š
        vocab_size = len(dataset.processor.char_to_id)
        config['model']['vocab_size'] = vocab_size
        
        print(f"ğŸ“š èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        print("\nğŸ—ï¸  ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
        model = HybridMambaModel(**config['model'])
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°è¡¨ç¤º
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"ğŸ’¼ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        print(f"ğŸ¯ å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆ
        print("\nğŸš€ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šä¸­...")
        trainer = ImprovedMambaTrainer(
            model=model,
            device=device,
            use_mixed_precision=not args.no_mixed_precision,
            use_compilation=not args.no_compile
        )
        
        # æœ€é©åŒ–å™¨è¨­å®š
        trainer.setup_optimizer(
            learning_rate=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay'],
            warmup_steps=config['training']['warmup_steps'],
            max_steps=config['training']['max_steps']
        )
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(config['save_dir'], exist_ok=True)
        
        # è¨­å®šã‚’ä¿å­˜
        config_path = os.path.join(config['save_dir'], 'config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # å­¦ç¿’é–‹å§‹
        print("\n" + "=" * 80)
        print("ğŸ¯ å­¦ç¿’é–‹å§‹")
        print("=" * 80)
        
        start_time = time.time()
        
        # å˜ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        train_dataloaders = {'main': train_dataloader}
        
        trainer.train(
            train_dataloaders=train_dataloaders,
            eval_dataloaders=None,  # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ
            max_steps=config['training']['max_steps'],
            gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
            eval_steps=config['training']['eval_steps'],
            save_steps=config['training']['save_steps'],
            logging_steps=config['training']['logging_steps'],
            save_dir=config['save_dir'],
            max_grad_norm=config['training']['max_grad_norm'],
            label_smoothing=0.1
        )
        
        training_time = time.time() - start_time
        
        print("\n" + "=" * 80)
        print("âœ… å­¦ç¿’å®Œäº†")
        print("=" * 80)
        print(f"â° ç·å­¦ç¿’æ™‚é–“: {training_time:.2f}ç§’ ({training_time/60:.1f}åˆ†)")
        
        # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        print("\nğŸª ç”Ÿæˆãƒ†ã‚¹ãƒˆ")
        print("-" * 40)
        
        model.eval()
        test_input = dataset.processor.encode(args.test_text, add_special_tokens=True)
        test_input = torch.tensor([test_input[:50]], dtype=torch.long).to(device)  # é•·ã•åˆ¶é™
        
        with torch.no_grad():
            generated = model.generate(
                test_input,
                max_length=min(100, config['model']['max_seq_len']),
                temperature=0.8,
                do_sample=True
            )
        
        generated_text = dataset.processor.decode(generated[0].cpu().tolist())
        print(f"å…¥åŠ›: {args.test_text}")
        print(f"ç”Ÿæˆ: {generated_text}")
        
        # å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        print("\nğŸ“ˆ å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ä¸­...")
        trainer.plot_advanced_training_curves(config['save_dir'])
        
        print(f"\nğŸ’¾ å…¨ã¦ã®çµæœãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {config['save_dir']}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  å­¦ç¿’ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†")


if __name__ == "__main__":
    main() 