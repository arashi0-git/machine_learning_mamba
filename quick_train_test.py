#!/usr/bin/env python3
"""
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ç°¡å˜ãªè¨“ç·´ãƒ†ã‚¹ãƒˆ
"""
import torch
import torch.optim as optim
import time
import json
from pathlib import Path

def create_simple_dataset():
    """ç°¡å˜ãªãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    # å°ã•ãªèªå½™ã§ã®ç°¡å˜ãªæ–‡ç« 
    sentences = [
        "hello world this is a test",
        "machine learning is amazing",
        "deep learning with transformers",
        "attention is all you need",
        "mamba model is efficient"
    ]
    
    # èªå½™ä½œæˆ
    vocab = set()
    for sentence in sentences:
        vocab.update(sentence.split())
    vocab = sorted(list(vocab))
    vocab_size = len(vocab)
    
    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    word_to_id = {word: i for i, word in enumerate(vocab)}
    
    tokenized_data = []
    for sentence in sentences:
        tokens = [word_to_id[word] for word in sentence.split()]
        tokenized_data.append(tokens)
    
    return tokenized_data, vocab_size, word_to_id

def simple_training_test():
    """ç°¡å˜ãªè¨“ç·´ãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ç°¡å˜è¨“ç·´ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    data, vocab_size, word_to_id = create_simple_dataset()
    print(f"èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(data)}")
    
    try:
        from hybrid_mamba_model import HybridMambaModel
        
        # å°ã•ãªãƒ¢ãƒ‡ãƒ«è¨­å®š
        model = HybridMambaModel(
            vocab_size=vocab_size,
            d_model=64,        # å°ã•ã
            n_layers=2,        # å°‘ãªã
            n_heads=4,         # å°‘ãªã
            num_experts=2,     # å°‘ãªã
            max_seq_len=32,
            dropout=0.1
        )
        
        model_info = model.get_model_info()
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_info['total_parameters']:,}")
        
        optimizer = optim.AdamW(model.parameters(), lr=1e-3)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™
        max_len = 8
        train_data = []
        for tokens in data:
            if len(tokens) > max_len:
                tokens = tokens[:max_len]
            else:
                tokens = tokens + [0] * (max_len - len(tokens))  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            train_data.append(tokens)
        
        # è¨“ç·´ãƒ«ãƒ¼ãƒ—
        model.train()
        losses = []
        
        print(f"\nğŸ“š è¨“ç·´é–‹å§‹ (10ã‚¹ãƒ†ãƒƒãƒ—)")
        for step in range(10):
            total_loss = 0
            for tokens in train_data:
                input_ids = torch.tensor([tokens], dtype=torch.long)
                labels = input_ids.clone()
                
                optimizer.zero_grad()
                outputs = model(input_ids, labels=labels)
                loss = outputs['loss']
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / len(train_data)
            losses.append(avg_loss)
            perplexity = torch.exp(torch.tensor(avg_loss)).item()
            
            print(f"ã‚¹ãƒ†ãƒƒãƒ— {step+1}: æå¤±={avg_loss:.4f}, PPL={perplexity:.2f}")
        
        # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ¯ ç”Ÿæˆãƒ†ã‚¹ãƒˆ")
        model.eval()
        test_input = torch.tensor([[word_to_id.get('hello', 0)]], dtype=torch.long)
        
        with torch.no_grad():
            generated = model.generate(test_input, max_length=8, temperature=0.8)
        
        # é€†ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        id_to_word = {i: word for word, i in word_to_id.items()}
        generated_words = [id_to_word.get(token.item(), '<UNK>') for token in generated[0]]
        print(f"ç”Ÿæˆæ–‡: {' '.join(generated_words)}")
        
        # çµæœã¾ã¨ã‚
        results = {
            'model_type': 'HybridMambaModel',
            'vocab_size': vocab_size,
            'model_params': model_info['total_parameters'],
            'training_steps': 10,
            'final_loss': losses[-1],
            'final_perplexity': torch.exp(torch.tensor(losses[-1])).item(),
            'loss_reduction': (losses[0] - losses[-1]) / losses[0] * 100,
            'generated_text': ' '.join(generated_words)
        }
        
        print(f"\nğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
        print(f"åˆæœŸæå¤±: {losses[0]:.4f} â†’ æœ€çµ‚æå¤±: {losses[-1]:.4f}")
        print(f"æå¤±æ¸›å°‘ç‡: {results['loss_reduction']:.1f}%")
        print(f"æœ€çµ‚ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£: {results['final_perplexity']:.2f}")
        
        # çµæœä¿å­˜
        with open('hybrid_training_test.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        return results
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = simple_training_test()
    if results:
        print(f"\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    else:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—")